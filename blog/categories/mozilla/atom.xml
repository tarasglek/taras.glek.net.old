<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: mozilla | All About Performance]]></title>
  <link href="http://taras.glek.net/blog/categories/mozilla/atom.xml" rel="self"/>
  <link href="http://taras.glek.net/"/>
  <updated>2014-05-07T16:19:24-07:00</updated>
  <id>http://taras.glek.net/</id>
  <author>
    <name><![CDATA[Taras Glek]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[More &amp; Faster C-I for Less on AWS]]></title>
    <link href="http://taras.glek.net/blog/2014/03/05/more-and-faster-c-i-for-less-on-aws/"/>
    <updated>2014-03-05T18:22:00-08:00</updated>
    <id>http://taras.glek.net/blog/2014/03/05/more-and-faster-c-i-for-less-on-aws</id>
    <content type="html"><![CDATA[### Amazon Pricing - Expensive or Cheap?
Amazon ondemand nodes are fantastic for rapid iteration, but using them in production is expensive naivety. It is expensive for Amazon to maintain spare capacity to allow customers to launch any of the wide variety of nodes they offer ondemand. Forecasing demand at Amazon scale can't be easy. As a result, Amazon recommends that customers buy reserves with an upfront payment then pay a discounted rate after. This is brilliant as it shifts the capacity-planning burden to each customer. This would net us a 60% discount if we we could forecast our AWS usage perfectly.

Fortunately Amazon also has a spot-pricing model. Spot prices can be 70-90% lower than ondemand (we've also seen them 50% higher). The downside is that Amazon can kill these nodes at any point and node availability is limited compared to ondemand. Given that Amazon competition can't match spot prices, Amazon might be selling their unused ondemand capacity at cost. I doubt that anyone smaller than Amazon can maintain their own hardware with salaried ops for less than Amazon's spot prices.

### Spot Everything

We spent 2014 retrofitting our c-i architecture to cope with failure so we can run more of our workload on spot nodes.

On our January AWS bill we were 30% more cost-efficient. This was accomplished late in the month, we managed to have the bill not go up to cope with a higher-than-ever load.
For February we were aiming to drop the bill to under $80K. 
The following is a summary of where we are.

### Provisioning

* We now run the majority of our workload on Amazon spot nodes. Ondemand:spot ratio is between 2:1 and 7:1. Note we still pay more for ondemand portion of our bill because ondemand is a lot more expensive
* At $74,389.03, our Feb bill is 36% lower than Jan.
* Our current AWS spending per job is approximately half of what we paid in December
* We now bid on a range of AWS node types to maximize node availability and minimize price. This results in  >=50% lower spot bill. We now run a portion of our workload on 2x-faster VMs when cheaper spot machine types are not available.

### Scheduling

* Our AWS scheduler ramps up slower now to avoid temporary overprovisioning. Note the improvement on the right side of the graph (tall & narrow  spikes are bad)

![catlee's graph](http://people.mozilla.org/~catlee/sattap/296cb846.png)

### Monitoring

* We are evaluating [hostedgraphite.com](http://hostedgraphite.com/) for monitoring our efficiency. It's nice to have someone offer a well-supported open-source-compatible solution that can cope with 30K+ of metrics our 1000s of VMs generate.

![](/assets/images/spotinstances.png "Spot Instance Types Used for C-I")

![](/assets/images/spotprice.png "Approximate Spot Prices")


### Workload Improvements

* Halved Linux Firefox and Android try build time via a shared [S3 object cache](http://glandium.org/blog/?p=3201)
* Halved B2G builds via [jacuzzis](http://atlee.ca/blog/posts/initial-jacuzzi-results.html)

### Mozilla Data Center plans for March

Amazon S3 is cheap, fast and robust. EC2 is incredibly flexible. Both are great for quickly iterating on cool ideas. Unfortunately most of our infrastructure runs on physical machines. We need to improve our non-elastic inhouse capacity with what we learned in the cloud:

* Use a shared object cache for Windows/Mac builds. This should more than double Windows build speed. The plan is to use Ceph for S3-compatible shared object storage.
* Get OpenStack bare metal virtualization working so we could move as fast there as we do in EC2

### Cloud Plans for March

* Eliminate EBS usage for faster builds, 10% lower EC2 bill. Amazon EBS is the antithesis of cost-effectiveness.
* Deploy more jacuzzis for faster builds, less EC2 instances
* Run more things on spot, switch to cheaper ondemand nodes, maybe buy some reserves
* Bid on an even wider variety of spot nodes
* Probably wont hit another 30% reduction, focusing on technical debt, better metrics, etc
* Containerization of Linux builds

### Conclusion

Cloud APIs make cost-oriented architectures fun. Batch nature of c-i is a great match for spot.

In general, spot is a brilliant design pattern, I intend to implement spot workloads on our own infra. It's too bad other cloud vendors do not offer anything comparable.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Cost-Efficient Continuous Integration]]></title>
    <link href="http://taras.glek.net/blog/2014/02/06/cost-efficient-continious-integration/"/>
    <updated>2014-02-06T13:16:00-08:00</updated>
    <id>http://taras.glek.net/blog/2014/02/06/cost-efficient-continious-integration</id>
    <content type="html"><![CDATA[As of a few weeks ago, Chris AtLee and his half of Mozilla Release engineering report to me.

We are working on drastically improving efficiency of our continuous integration infrastructure. 

For 2014 our focus is on cost-efficiency, developer efficiency is also a priority. Mozilla has a few thousand (around 3000, including various ARM boards) real machines dedicated to building & testing our products. There are an additional ~500-1000 virtual machines on Amazon EC2. We have been spending around ~$115K per month for Amazon AWS infra for the past 4 months (e.g. Linux/B2G/Android builds & unit tests. Linux perf tests run on real hw). It's harder to get numbers for our internal costs, but a conservative estimate would be to  assume that Amazon AWS is 1/3rd of our spend given that we test Windows, Mac, Android, etc in-house. 

Number of c-i hours goes up with every month, so it is crucial to get a grip on our infrastructure efficiency in terms of cost per job.

My cloud target is to get our current Amazon EC2 workload to under $30K/month by the end of the year. This will give us room to move Mac builds, and Windows unit-tests + compiles into the cloud.
 
Where to save money?
One can break down the c-i process into 3+1 distinct optimization areas:

1. Building & Testing Workloads
2. Scheduling Logic
3. Provisioning Bare Metal or Virtual Machines
4. Reporting

Items 1-3 are ordered in terms of dependencies where in theory #1 does not care how it's scheduled or what machines it's run on(#3).  #3 is what we get billed for and it is largely a result of #1 and #2.

\#4 is mainly a human cost in that we don't have a lot of insight into how efficiently our jobs run. Making our systems more transparent should make optimization easier.

In terms of work:

1. We made great progress with reducing build times in 2013. Compilation is the biggest contributor to our EC2 bill. We have more work to do there. We'll focus on optimizing tests to run more efficiently once build costs are on-par in cost terms.
2. The way we use Buildbot results in a lot of idle-time on machines. There is a lot of room for improvements via containerization, running more jobs in parallel. We are looking at potentially replacing Buildbot with a simpler and more cloud-friendly framework (TaskCluster).
3. We saved $30K in January by tweaking what kind of EC2 nodes we use. We switched to more cost-efficient ondemand nodes (m3.xlarge -> c3.xlarge, m1.medium -> m3.medium) and started running try jobs and tests on EC2 spot nodes.

### Moving to Public Clouds
We will be moving all of our Mac/Win/Linux (other than perf-testing) infrastructure into the cloud. Windows, Linux and most of Mac builds and tests should run on hardware outside of  Mozilla data-centers by the end of 2014. Cloud infra like EC2 is expensive for inefficient workloads. We will be optimizing workloads locally until they can run on a public cloud in a cost-efficient manner (eg VMs with little idle capacity or redundant tasks).
I expect cloud infra to save Mozilla developer time and money.

### Private Cloud Functionality
For obvious reasons, we are stuck doing performance tests and many Mac workloads on our bare metal. Unfortunately suitable bare metal public clouds do not exist yet.
We would like to approach EC2-like engineering efficiencies by reconfiguring machines in our data center as a private bare metal cloud based on OpenStack. We will be replacing bugzilla+human  intervention with APIs for reimaging, slave  loans, etc. 
Ideally all of our pc, mac, arm onsite,offsite hardware would be controlled by the same API. 
I hope that in addition to making our hardware ops happier, opening up machine configuration via self-serve access control will greatly increase our ability to iterate on improving machine configurations, bringing new configurations online, etc.

### Spot workloads
Amazon's spot infra (http://aws.amazon.com/ec2/purchasing-options/spot-instances/)  is a great idea for minimizing idle time on machines.
I would like to have spot workloads in public and private clouds. Try and fuzzing deal well with being interrupted and soaking up spare capacity. 

### Developers with keys
The releng team is small. Unless you have a critical request, expect us to hear a "no, we are too busy" followed by "but here are the keys to our test public/private clouds, go do what you want and we'll help you deploy it". I hope that Mozilla developers will appreciate not having to block on busy intermediaries and help themselves.  

My strategy is largely inspired by this [Forrester analysis](http://www.youtube.com/watch?v=1LRSS29SH44). Infrastructure devop teams should strive to enable other teams to self-serve. Our role is to help developers do things right, not to be gatekeepers or do work that developers find unsatisfying.

### How to Get Involved

In 2014 I'd like to move our c-i from a 90s-style architecture with a few modern cloud components to a modern cloud-oriented architecture. The following areas might be fun to help with:

* We need to be more data-driven. Need to process, visualize and correlate various logs to direct optimization decisions. We have build logs, buildbot logs, AWS usage logs, AWS Spot price logs, etc. 
* Bare metal virtualization with OpenStack for Mac, Windows, ARM boards is an open problem
* Containerizing builds, tests via Docker/LXC.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Role: Developer Productivity]]></title>
    <link href="http://taras.glek.net/blog/2013/11/20/new-role-developer-productivity/"/>
    <updated>2013-11-20T13:44:00-08:00</updated>
    <id>http://taras.glek.net/blog/2013/11/20/new-role-developer-productivity</id>
    <content type="html"><![CDATA[New Role
--------
The performance part of my team is now lead by [Vladan](http://blog.mozilla.org/vdjeric/). They are doing usual performancy things with a focus on improving our 'internal' benchmarks such as talos, TART + some upcoming power testing. 

I got a new task recently: developer productivity. I'm prioritizing tasks based on developer unproductivity. I have the following new stuff on my radar:

* slow build system for local builds
* long build/test times on our release automation
* bugzilla improvements (eg tracking review times, lack of pull reqs, etc)

Expect to hear less Gecko and more web dev stuff out of me for the foreseable future. Note, this is a collaborative project. My team would not be able to accomplish any of the new goals as we are already above capacity. I'm mainly working on improving coordination and collaboration on existing projects. Let me know if you have ideas related to this dev productivity stuff.


Cost-Oriented Architecture
---------------------------
My main project for the remainder of this year is to maximize throughput per dollar spent on our release automation infrastructure. I'm focusing on computation that happens on Amazon EC2(because it's easier than pricing out our own infrastructure). My goal is to be able to say "making improvement x will pay for itself in y time". This is mainly management technique to make it easier to justify working on infrastructure.

John posted some initial costing [figures](http://oduinn.com/blog/2013/11/20/the-financial-cost-of-a-checkin-part-1/). Rail is working on switching us over to Amazon Spot instances in <a title="Prepare infra to handle spot instances" href="https://bugzilla.mozilla.org/show_bug.cgi?id=935533">bug 935533</a>. I expect a 2-5x reduction in our AWS costs once Rail is done.

I'd like to have a receipt attached to every job that executes on our build infrastructure. I hope this encourages devs to look into build/test inefficiencies. 

Keeping an eye on costs means we'll be able to better reason about moving more workloads into the cloud. Cloud stuff is attractive because it allows us to give developers keys to AWS. They can deploy(in a low-friction manner) whatever they need to make their job easier while cost-monitoring ensures that things don't get crazy.

Open Architecture
------------------
In theory, open source is about scratching own needs. In practice service owners (usually inadvertently) put up walls to contribution by depending on licensing requirements, poor documentation, vpns, obscure infrastructure, weird version control, etc. This then snowballs into something equivalent to closed source where 'clients' have to ask for 'features' from 'owners'. This usually results in unhappy customers and overworked owners. This happened to Telemetry (see previous post) and is happening to a few of our tools. Expect improvements in this area.

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Telemetry Reboot]]></title>
    <link href="http://taras.glek.net/blog/2013/08/28/telemetry-reboot/"/>
    <updated>2013-08-28T15:54:00-07:00</updated>
    <id>http://taras.glek.net/blog/2013/08/28/telemetry-reboot</id>
    <content type="html"><![CDATA[A year ago I wrote 2 blog posts describing telemetry ([post1](/blog/2012/07/25/telemetry-and-what-it-is-good-for-part-2-telemetry-achievements/), [post2](/blog/2012/07/25/telemetry-and-what-it-is-good-for-part-1-nuts-and-bolts/)).

What sucks about telemetry?
---------------------------
The following aspects of performance made us unhappy with telemetry as deployed now: dashboard performance, effort required to add new dashboards/visualizations, data-gathering latency and slow ad-hoc queries on hadoop. Telemetry dashboards keep timing out, infrastructure keeps failing, this can't be the way to drive Firefox performance improvements.

Telemetry Reboot
-----------------
After a few months of soul-searching we ended up with a small group of people working on a replacement for current telemetry serverside. The goals are to minimize latency in the following dimensions:

* Serverside should be fast and robust. Slow dashboards are a good way to discourage their use. 
* We should have extensive monitoring and alerting to notify us when data isn't coming in (eg due to a bug in Firefox client code).
* Everything should be open source and easy to hack on. Outgoing telemetry infrastructure has an overly complicated software stack and code isn't open source. This was a mistake.
* Fast ad-hoc queries
* Minimal submission->graphing latency. Currently it takes about 3 days between landing a piece of telemetry code and having that data show up in a dashboard. Takes about 7 to gather a useful amount of data. My goal is to eventually do live-histogram-aggregation so the data can be graphed as soon as it hits the server. This means we should be able to gather a useful amount of telemetry data within 3 days of landing a probe. Should we choose to switch to shipping hourly nightlies, telemetry would come back even faster :)

ETA
---
I expect to shut down old dashboards and cut over to the new server + dashboards on Oct 1. See the wiki for our next set of [milestones](https://wiki.mozilla.org/Telemetry/Reboot).

We'll be deploying the new telemetry backend on Amazon AWS. I'll write a blog post about switching from a private cluster to AWS once we go live.

[Mark](http://mreid-moz.github.io/) is working on our serverside. Mark started implementing the server in Python/Django, but Node.js turned out to be a much more potent platform in terms of amount of work needed to squeeze out good performance. You can find the current node [code](https://github.com/mreid-moz/telemetry-server/blob/master/server/server.js) on github. I suspect we'll be switching to a C/C++ HTTP receiver in the future to get another 2-3x increase in req/s performance. In the past we've run into servers having difficulty coping with increases in telemetry volume (submissions went up 10-fold when we added saved-session telemetry).

At the moment we are looking at squeezing out the maximum possible requests per second out of node. If you have a background of that sort of thing feel free to try improving our code.

I spent a lot of time thinking about how to write the fastest(+robustest) possible telemetry dashboard. I ended up writing a [dashboard](http://telemetry-dash.mozilla.org/) ([code](https://github.com/mozilla/telemetry-dashboard/)) based on static JSON. This approach is fast and should make dashboards easy to contribute to. [Chris](http://xor.lonnen.com/) is working on making my prototype useful and usable. The dashboard is going to be going through a lot of refactoring in the near future. At the moment we use [jydoop](https://github.com/mozilla/jydoop) to generate the dashboard JSON from hadoop (was easier to write a new query tool for hadoop than do a complex map/reduce in Java).

Summary
-------
We will be cutting over to brand new telemetry infrastructure on Oct 1. There will be pain. If you have any questions or are interested in helping out ping us on IRC in `#telemetry`.
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[New Performance People, Telemetry News]]></title>
    <link href="http://taras.glek.net/blog/2013/06/28/new-performance-people/"/>
    <updated>2013-06-28T16:52:00-07:00</updated>
    <id>http://taras.glek.net/blog/2013/06/28/new-performance-people</id>
    <content type="html"><![CDATA[reddit/[MozillaTech](http://www.reddit.com/r/MozillaTech/)
--------------------------------
I like our make-planet-techy subreddit. Seems like articles that get rated as one would expect. I happily switched to reddit as my primary way to consume planet mozilla. Redditors, keep up the good work!

Telemetry Dashboard
-------------------------------
Our new telemetry [dashboard](http://telemetry-dash.mozilla.org/) went live yesterday. It's missing features, data, UX. However it is public, fast and hackable. It should evolve quickly.
New Blood
---------------
[Mark Reid](http://mreid-moz.github.io/) is our first server-side dev. His primary task is switching telemetry backend from hadoop to a custom telemetry server. New server infrastructure will enable a live dashboard (3-day delay atm) + ability to run queries in minutes rather than in hours.

[Dhaval Giani](http://randomkernels.wordpress.com), "the intern", is our first kernel hacker. He's working on helping land [volatile memory](https://lwn.net/Articles/522135/) in the kernel. This feature will let Firefox safely consume more memory when memory is plentiful and use less in limited scenarios. Hopefully he'll also add read-only file compression to ext4.

Blogs linked above are in the please-add-to-planet queue. Expect them to spend a few weeks there. Please subscribe to their RSS feeds in meantime.
 
Investigating SQLite Performance via Telemetry
----------------------------------------------------------------------
Years ago I tweaked our SQLite clients to use a larger page size. In my testing this seemed to achieve a speedup of 0.2-2x, see <a title="Suboptimal SQLite page size" href="https://bugzilla.mozilla.org/show_bug.cgi?id=416330">bug 416330</a>. Unfortunately, ~30% of our users are still on the old page size (<a title="Fix page_size of WAL databases" href="https://bugzilla.mozilla.org/show_bug.cgi?id=634374">bug 634374</a>). We used a [jydoop](https://github.com/mozilla/jydoop) query to figure out if it's worth developing a feature to convert these users over using two different (time spent [reading](https://github.com/mozilla/jydoop/blob/master/scripts/places4k.py) & time spent [executing](https://github.com/mozilla/jydoop/blob/master/scripts/places4k-slowsql.py) a query) ways to measure performance differences. According to both methods there is a 4x reduction in sqlite IO waits with the larger page size, so we'll be adding code to convert people over more aggressively.

This was a cool investigation because it highlighted how much more confidence we have when making performance decisions now (due to telemetry) vs a few years ago.
]]></content>
  </entry>
  
</feed>
